{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Review Classification\n",
    "\n",
    "## Business Understanding\n",
    "Our company wants a tool that will automatically classify product reviews as _positive_ or _negative_ reviews, based on the features of the review.  This will help our Product team to perform more sophisticated analyses in the future to help ensure customer satisfaction.\n",
    "\n",
    "## Data Understanding\n",
    "We have a labeled collection of 20,000 product reviews, with an equal split of positive and negative reviews. The dataset contains the following features:\n",
    "\n",
    " - `ProductId` Unique identifier for the product\n",
    " - `UserId` Unqiue identifier for the user\n",
    " - `ProfileName` Profile name of the user\n",
    " - `HelpfulnessNumerator` Number of users who found the review helpful\n",
    " - `HelpfulnessDenominator` Number of users who indicated whether they found the review helpful or not\n",
    " - `Time` Timestamp for the review\n",
    " - `Summary` Brief summary of the review\n",
    " - `Text` Text of the review\n",
    " - `PositiveReview` 1 if this was labeled as a positive review, 0 if it was labeled as a negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>PositiveReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B002QWHJOU</td>\n",
       "      <td>A37565LZHTG1VH</td>\n",
       "      <td>C. Maltese</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1305331200</td>\n",
       "      <td>Awesome!</td>\n",
       "      <td>This is a great product. My 2 year old Golden ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000ESLJ6C</td>\n",
       "      <td>AMUAWXDJHE4D2</td>\n",
       "      <td>angieseashore</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1320710400</td>\n",
       "      <td>Was there a recipe change?</td>\n",
       "      <td>I have been drinking Pero ever since I was a l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B004IJJQK4</td>\n",
       "      <td>AMHHNAFJ9L958</td>\n",
       "      <td>A M</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1321747200</td>\n",
       "      <td>These taste so bland.</td>\n",
       "      <td>Look, each pack contains two servings of 120 c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId          UserId    ProfileName  HelpfulnessNumerator  \\\n",
       "0  B002QWHJOU  A37565LZHTG1VH     C. Maltese                     1   \n",
       "1  B000ESLJ6C   AMUAWXDJHE4D2  angieseashore                     1   \n",
       "2  B004IJJQK4   AMHHNAFJ9L958            A M                     0   \n",
       "\n",
       "   HelpfulnessDenominator        Time                     Summary  \\\n",
       "0                       1  1305331200                    Awesome!   \n",
       "1                       1  1320710400  Was there a recipe change?   \n",
       "2                       1  1321747200       These taste so bland.   \n",
       "\n",
       "                                                Text  PositiveReview  \n",
       "0  This is a great product. My 2 year old Golden ...               1  \n",
       "1  I have been drinking Pero ever since I was a l...               0  \n",
       "2  Look, each pack contains two servings of 120 c...               0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"reviews.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has already been cleaned, so there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductId                 0\n",
       "UserId                    0\n",
       "ProfileName               0\n",
       "HelpfulnessNumerator      0\n",
       "HelpfulnessDenominator    0\n",
       "Time                      0\n",
       "Summary                   0\n",
       "Text                      0\n",
       "PositiveReview            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PositiveReview` is the target, and all other columns are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"PositiveReview\", axis=1)\n",
    "y = df[\"PositiveReview\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "First, split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, prepare for modeling. The following `Pipeline` prepares all data for modeling.  It one-hot encodes the `ProductId`, applies a tf-idf vectorizer to the `Summary` and `Text`, keeps the numeric columns as-is, and drops all other columns.\n",
    "\n",
    "The following code may take up to 1 minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 11275)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_irrelevant_columns(X):\n",
    "    return X.drop([\"UserId\", \"ProfileName\"], axis=1)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"drop_columns\", FunctionTransformer(drop_irrelevant_columns)),\n",
    "    (\"transform_text_columns\", ColumnTransformer(transformers=[\n",
    "        (\"ohe\", OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\", sparse=False), [\"ProductId\"]),\n",
    "        (\"summary-tf-idf\", TfidfVectorizer(max_features=1000), \"Summary\"),\n",
    "        (\"text-tf-idf\", TfidfVectorizer(max_features=1000), \"Text\")\n",
    "    ], remainder=\"passthrough\"))\n",
    "])\n",
    "\n",
    "X_train_transformed = pipeline.fit_transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Fit a `RandomForestClassifier` with the best hyperparameters.  The following code may take up to 1 minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=30, min_samples_split=15, random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_depth=30,\n",
    "    min_samples_split=15,\n",
    "    min_samples_leaf=1\n",
    ")\n",
    "rfc.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We are using _accuracy_ as our metric, which is the default metric in Scikit-Learn, so it is possible to just use the built-in `.score` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9846666666666667\n",
      "Test accuracy: 0.9116\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", rfc.score(X_train_transformed, y_train))\n",
    "print(\"Test accuracy:\", rfc.score(X_test_transformed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train confusion matrix:\n",
      "[[7323  166]\n",
      " [  64 7447]]\n",
      "Test confusion matrix:\n",
      "[[2286  225]\n",
      " [ 217 2272]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train confusion matrix:\")\n",
    "print(confusion_matrix(y_train, rfc.predict(X_train_transformed)))\n",
    "print(\"Test confusion matrix:\")\n",
    "print(confusion_matrix(y_test, rfc.predict(X_test_transformed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Data Preparation\n",
    "\n",
    "A train-test split has already been performed.\n",
    "\n",
    "Additionally, there is already a pipeline in place that drops some columns and converts all text columns into a numeric format for modeling.\n",
    "\n",
    "**Your only additional data preparation task is feature scaling.**  Tree-based models like Random Forest Classifiers do not require scaling, but TensorFlow neural networks do.\n",
    "\n",
    "There are two main strategies you can take for this task:\n",
    "\n",
    "#### Scaling within the existing pipeline\n",
    "\n",
    "If you are comfortable with pipelines, this is the more polished/professional route.\n",
    "\n",
    "1. Make a new pipeline, with a `StandardScaler` as the final step.  You can nest the steps of the previous pipeline inside of this new pipeline\n",
    "2. Generate a new `X_train_transformed_scaled` by calling `.fit_transform` on the new pipeline\n",
    "3. Generate a new `X_test_transformed_scaled` by calling `.transform` on the new pipeline\n",
    "\n",
    "#### Scaling after the pipeline has finished\n",
    "\n",
    "This is a better strategy if you are not as comfortable with pipelines.\n",
    "\n",
    "1. Instantiate a `StandardScaler` object\n",
    "2. Generate a new `X_train_transformed_scaled` by calling `.fit_transform` on the scaler object, after you have called `.fit_transform` on the pipeline\n",
    "3. Generate a new `X_test_transformed_scaled` by calling `.transform` on the scaler object, after you have called `.transform` on the pipeline\n",
    "\n",
    "If you are getting stuck at this step, skip it.  The model will still be able to fit, although the performance will be worse.  Keep in mind whether or not you scaled the data in your final analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = pipeline.fit_transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "ss=StandardScaler()\n",
    "\n",
    "X_train_transformed_scaled= ss.fit_transform(X_train_transformed)\n",
    "X_test_transformed_scaled= ss.transform(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 11275)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5514     1\n",
       "1266     0\n",
       "5864     1\n",
       "15865    1\n",
       "12892    1\n",
       "        ..\n",
       "11284    1\n",
       "11964    0\n",
       "5390     1\n",
       "860      1\n",
       "15795    0\n",
       "Name: PositiveReview, Length: 15000, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train # our outputs for target are already binarized. no need for further transformations here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Modeling\n",
    "\n",
    "Build a neural network classifier.  Specifically, use the `keras` submodule of the `tensorflow` library to build a multi-layer perceptron model with the `Sequential` interface.\n",
    "\n",
    "See the [`tf.keras` documentation](https://www.tensorflow.org/guide/keras/overview) for an overview on the use of `Sequential` models. See the [Keras layers documentation](https://keras.io/layers/core/) for descriptions of the `Dense` layer options.  \n",
    "\n",
    "1. Instantiate a `Sequential` model\n",
    "2. Add an input `Dense` layer.  You'll need to specify a `input_shape` = (11275,) because this is the number of features of the transformed dataset.\n",
    "3. Add 2 `Dense` hidden layers.  They can have any number of units, but keep in mind that more units will require more processing power.  We recommend an initial `units` of 64 for processing power reasons.\n",
    "4. Add a final `Dense` output layer.  This layer must have exactly 1 unit because we are doing a binary prediction task.\n",
    "5. Compile the `Sequential` model\n",
    "6. Fit the `Sequential` model on the preprocessed training data (`X_train_transformed_scaled`) with a b`batch_size` of 50 and `epochs` of 5 for processing power reasons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1= Sequential() #instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.add(Dense(units=110, activation='relu', input_shape= (X_train_transformed_scaled.shape[1],))) #input layer\n",
    "model_1.add(Dense(units=64)) #hidden layer\n",
    "model_1.add(Dense(units=64)) #hidden layer\n",
    "model_1.add(Dense(units=1, activation='sigmoid',)) # output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics =['accuracy']) #compiling the model and setting our metrics\n",
    "#for accuracy, and our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "300/300 [==============================] - 3s 8ms/step - loss: 0.2849 - accuracy: 0.8791\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 3s 8ms/step - loss: 0.0237 - accuracy: 0.9916\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0068 - accuracy: 0.9976\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 3s 10ms/step - loss: 0.0042 - accuracy: 0.9988\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 3s 9ms/step - loss: 8.9712e-04 - accuracy: 0.9997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1acb2b7fe50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(X_train_transformed_scaled, y_train, batch_size= 50, epochs= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 110)               1240360   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                7104      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,251,689\n",
      "Trainable params: 1,251,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-18ac34470bfc>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "y_hat= model_1.predict_classes(X_test_transformed_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, recall_score\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2275,  236],\n",
       "       [ 238, 2251]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9052"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_model1= accuracy_score(y_test,y_hat)\n",
    "acc_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performs about just as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Model Tuning + Feature Engineering\n",
    "\n",
    "If you are running out of time, skip this step.\n",
    "\n",
    "Tune the neural network model to improve performance.  This could include steps such as increasing the units, changing the activation functions, or adding regularization.\n",
    "\n",
    "We recommend using using a `validation_split` of 0.1 to understand model performance without utilizing the test holdout set.\n",
    "\n",
    "You can also return to the preprocessing phase, and add additional features to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "#from tensorflow.keras import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "300/300 [==============================] - 3s 12ms/step - loss: 0.7524 - accuracy: 0.8968\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3305 - accuracy: 0.9685\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.2665 - accuracy: 0.9751\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.2407 - accuracy: 0.9799\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.2102 - accuracy: 0.9839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1acb9a19f40>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concerned that the model may be overfitting so I'm going to add a regularization layer, dropout\n",
    "# with a dropout rate of 0.3.\n",
    "\n",
    "#tensorflow did not let me play with parameters so im going to play with activation functions and units, regularizers\n",
    "\n",
    "model_2= Sequential()\n",
    "model_2.add(Dense(units=100, activation='relu', kernel_regularizer= regularizers.l2(0.005), input_shape=(X_train_transformed_scaled.shape[1],)))\n",
    "\n",
    "model_2.add(Dense(units=64, activation= 'relu',kernel_regularizer= regularizers.l2(0.005)))\n",
    "\n",
    "model_2.add(Dense(units=32, activation='relu',kernel_regularizer= regularizers.l2(0.005)))\n",
    "model_2.add(Dense(units=1, activation='sigmoid'))\n",
    "model_2.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model_2.fit(X_train_transformed_scaled, y_train, batch_size= 50, epochs= 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2292,  219],\n",
       "       [ 227, 2262]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_2= model_2.predict_classes(X_test_transformed_scaled)\n",
    "confusion_matrix(y_test,y_hat_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2292,  219],\n",
       "       [ 227, 2262]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_hat_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9108\n",
      "0.9052\n"
     ]
    }
   ],
   "source": [
    "acc_model2 =accuracy_score(y_test, y_hat_2)\n",
    "print(acc_model2)\n",
    "print(acc_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 100)               1127600   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,136,177\n",
      "Trainable params: 1,136,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.7608 - accuracy: 0.8971\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.3179 - accuracy: 0.9727\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.2631 - accuracy: 0.9751\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2532 - accuracy: 0.9778\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.2363 - accuracy: 0.9829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1acbddc2790>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3= Sequential()\n",
    "model_3.add(Dense(units=100, activation='relu', kernel_regularizer= regularizers.l2(0.005), input_shape=(X_train_transformed_scaled.shape[1],)))\n",
    "\n",
    "model_3.add(Dense(units=64, activation='tanh',kernel_regularizer= regularizers.l2(0.005)))\n",
    "\n",
    "model_3.add(Dense(units=32, activation='tanh',kernel_regularizer= regularizers.l2(0.005)))\n",
    "model_3.add(Dense(units =1, activation='sigmoid'))\n",
    "model_3.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model_3.fit(X_train_transformed_scaled, y_train, batch_size= 50, epochs= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9052 0.9108 0.9096\n"
     ]
    }
   ],
   "source": [
    "y_hat_3 = model_3.predict_classes(X_test_transformed_scaled)\n",
    "acc_model3= accuracy_score(y_test, y_hat_3)\n",
    "print(acc_model1, acc_model2, acc_model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 2.1381 - accuracy: 0.6826\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 1.6874 - accuracy: 0.9369\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 1.4515 - accuracy: 0.9883\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 1.3374 - accuracy: 0.9965\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 1.2518 - accuracy: 0.9985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1acbe14d970>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4= Sequential()\n",
    "model_4.add(Dense(units=100, activation='relu', kernel_regularizer= regularizers.l2(0.005), input_shape=(X_train_transformed_scaled.shape[1],)))\n",
    "\n",
    "model_4.add(Dense(units=64, activation='tanh',kernel_regularizer= regularizers.l2(0.005)))\n",
    "\n",
    "model_4.add(Dense(units=32, activation='tanh',kernel_regularizer= regularizers.l2(0.005)))\n",
    "model_4.add(Dense(units =1, activation='sigmoid'))\n",
    "model_4.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='sgd')\n",
    "model_4.fit(X_train_transformed_scaled, y_train, batch_size= 50, epochs= 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9052 0.9108 0.9096 0.9048\n"
     ]
    }
   ],
   "source": [
    "y_hat_4 = model_4.predict_classes(X_test_transformed_scaled)\n",
    "acc_model4= accuracy_score(y_test, y_hat_4)\n",
    "print(acc_model1, acc_model2, acc_model3, acc_model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.7576 - accuracy: 0.8707\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3276 - accuracy: 0.9662\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.2624 - accuracy: 0.9753\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2320 - accuracy: 0.9807\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2055 - accuracy: 0.9842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1acc0f0ef40>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5= Sequential()\n",
    "model_5.add(Dense(units=100, activation='relu', kernel_regularizer= regularizers.l2(0.005), input_shape=(X_train_transformed_scaled.shape[1],)))\n",
    "model_5.add(Dense(units=64))\n",
    "model_5.add(Dense(units=64, activation='relu',kernel_regularizer= regularizers.l2(0.005)))\n",
    "model_5.add(Dense(units=32, activation='relu'))\n",
    "model_5.add(Dense(units=32, activation='relu',kernel_regularizer= regularizers.l2(0.005)))\n",
    "model_5.add(Dense(units=16, activation='relu'))\n",
    "model_5.add(Dense(units=8, activation='relu',kernel_regularizer= regularizers.l2(0.005)))\n",
    "model_5.add(Dense(units =1, activation='sigmoid'))\n",
    "model_5.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model_5.fit(X_train_transformed_scaled, y_train, batch_size= 50, epochs= 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9052 0.9108 0.9096 0.9048 0.9094\n"
     ]
    }
   ],
   "source": [
    "y_hat_5 = model_5.predict_classes(X_test_transformed_scaled)\n",
    "acc_model5= accuracy_score(y_test, y_hat_5)\n",
    "print(acc_model1, acc_model2, acc_model3, acc_model4, acc_model5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Model Evaluation\n",
    "\n",
    "Choose a final `Sequential` model, add layers, and compile.  Fit the model on the preprocessed training data (`X_train_transformed_scaled`, `y_train`) and evaluate on the preprocessed testing data (`X_test_transformed_scaled`, `y_test`) using `accuracy_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.2177 - accuracy: 0.9841 - val_loss: 0.4699 - val_accuracy: 0.9106\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2153 - accuracy: 0.9841 - val_loss: 0.4456 - val_accuracy: 0.9104\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.1927 - accuracy: 0.9867 - val_loss: 0.4401 - val_accuracy: 0.9122\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.1617 - accuracy: 0.9905 - val_loss: 0.4513 - val_accuracy: 0.9096\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.1588 - accuracy: 0.9879 - val_loss: 0.4360 - val_accuracy: 0.9096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1acc3f4e5b0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(X_train_transformed_scaled, y_train, batch_size=50, validation_data=(X_test_transformed_scaled, y_test), epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_final= model_3.predict_classes(X_test_transformed_scaled) # testing final model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9096"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model_accuracy= accuracy_score(y_test, y_hat_final) #final model accuracy\n",
    "final_model_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Technical Communication\n",
    "\n",
    "Write a paragraph explaining whether Northwind Trading Company should switch to using your new neural network model, or continue to use the Random Forest Classifier.  Beyond a simple comparison of performance, try to take into consideration additional considerations such as:\n",
    "\n",
    " - Computational complexity/resource use\n",
    " - Anticipated performance on future datasets (how might the data change over time?)\n",
    " - Types of mistakes made by the two kinds of models\n",
    "\n",
    "You can make guesses or inferences about these considerations.\n",
    "\n",
    "**Include at least one visualization** comparing the two types of models.  Possible points of comparison could include ROC curves, colorized confusion matrices, or time needed to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "y_rfc_hat=rfc.predict(X_test_transformed) #storing rfc predictions to compare accuracy between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2286,  225],\n",
       "       [ 217, 2272]], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, rfc.predict(X_test_transformed)) # confusion matrix showing accuracy perfomance for rfc model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2216,  295],\n",
       "       [ 172, 2317]], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_hat_3) # confusion matrix showing accuracy performance for our final neural network model. model 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall both models perform very similarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9116\n",
      "0.9096\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, rfc.predict(X_test_transformed))) #RandomForest Accuracy score \n",
    "print(final_model_accuracy) # neural network accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the accuracy scores obtained from both the random forest classifier and the final neural network model being within 1% of eachother I would initally recommend telling Northwind Trading Company to stick with their original machine learning model. The reason being that it may not be worth their time to invest in a neural network model that performs just as well, but is not able to communicate to analysts which features they might focus on when it comes to correctly determining what helps the model classify a good review from a bad review. \n",
    "\n",
    "This being sad, the computation time for a random forest classifier can be very demanding. What you make up for in not having to scale features, you lose when it goes thru each random iteration of its decision tree. Taking this problem into context, correctly identifying a positive review from a negative review, it might not matter to us as much to identify what parameters lead the model to accurate identification. In this case, I would highly suggest the neural network.\n",
    "\n",
    "The neural network appears to be computationally faster, which is important if the Northwind Trading Company wants to go thru a even bigger dataset. My suggestion would be to use a Neural Network to correctly identify good reviews from bad reviews. Then retrieving a dataset about what the reviewer bought, how they bought it (online vs in-store), and how they retrieved it(via mail-carrier vs in store pick up), and then run a random forest classifier on what features of this process lead to positive and negative reviews, so they can be identified and respectively fixed or exalted. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
